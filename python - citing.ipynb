{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62fc243e",
   "metadata": {},
   "source": [
    "This notebook shows how crossreferences can be explored in python.\n",
    "\n",
    "As a preparatory step you need to install \"crossrefapi\":\n",
    "\n",
    "> pip install crossrefapi\n",
    "\n",
    "A good introduction demo is https://pypi.org/project/crossrefapi/1.0.3/, here we focus at the main funcionality.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244d6c9f",
   "metadata": {},
   "source": [
    "To start with we set up the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff30123b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossref.restful import Works\n",
    "works = Works()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28729fc6",
   "metadata": {},
   "source": [
    "Choose a DOI and retrive the article information.\n",
    "\n",
    "Here you might change to an \"own\" DOI\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea7b686d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "DOI = '10.1017/jfm.2017.541'\n",
    "DOI = '10.1007/978-94-007-0910-2_18' # Stillman; book !\n",
    "DOI = '10.1007/s10463-019-00720-8'\n",
    "DOI = '10.1016/j.chb.2019.01.037' # Natalia\n",
    "# DOI = '10.22937/ijcsns.2022.22.4.37' # Felipe\n",
    "DOI = '10.1109/ISC253183.2021.9562857' # Cyclist Trajectory Forecasts by Incorporation of Multi-View Video Information\n",
    "\n",
    "dct = works.doi(DOI)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f154bc71",
   "metadata": {},
   "source": [
    "An readable output format is as a json object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bc9d8b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"DOI\": \"10.1109/isc253183.2021.9562857\",\n",
      "    \"URL\": \"http://dx.doi.org/10.1109/isc253183.2021.9562857\",\n",
      "    \"author\": [\n",
      "        {\n",
      "            \"affiliation\": [],\n",
      "            \"family\": \"Zernetsch\",\n",
      "            \"given\": \"Stefan\",\n",
      "            \"sequence\": \"first\"\n",
      "        },\n",
      "        {\n",
      "            \"affiliation\": [],\n",
      "            \"family\": \"Trupp\",\n",
      "            \"given\": \"Oliver\",\n",
      "            \"sequence\": \"additional\"\n",
      "        },\n",
      "        {\n",
      "            \"affiliation\": [],\n",
      "            \"family\": \"Kress\",\n",
      "            \"given\": \"Viktor\",\n",
      "            \"sequence\": \"additional\"\n",
      "        },\n",
      "        {\n",
      "            \"affiliation\": [],\n",
      "            \"family\": \"Doll\",\n",
      "            \"given\": \"Konrad\",\n",
      "            \"sequence\": \"additional\"\n",
      "        },\n",
      "        {\n",
      "            \"affiliation\": [],\n",
      "            \"family\": \"Sick\",\n",
      "            \"given\": \"Bernhard\",\n",
      "            \"sequence\": \"additional\"\n",
      "        }\n",
      "    ],\n",
      "    \"container-title\": [\n",
      "        \"2021 IEEE International Smart Cities Conference (ISC2)\"\n",
      "    ],\n",
      "    \"content-domain\": {\n",
      "        \"crossmark-restriction\": false,\n",
      "        \"domain\": []\n",
      "    },\n",
      "    \"created\": {\n",
      "        \"date-parts\": [\n",
      "            [\n",
      "                2021,\n",
      "                10,\n",
      "                16\n",
      "            ]\n",
      "        ],\n",
      "        \"date-time\": \"2021-10-16T10:05:05Z\",\n",
      "        \"timestamp\": 1634378705000\n",
      "    },\n",
      "    \"deposited\": {\n",
      "        \"date-parts\": [\n",
      "            [\n",
      "                2022,\n",
      "                5,\n",
      "                10\n",
      "            ]\n",
      "        ],\n",
      "        \"date-time\": \"2022-05-10T15:47:22Z\",\n",
      "        \"timestamp\": 1652197642000\n",
      "    },\n",
      "    \"event\": {\n",
      "        \"end\": {\n",
      "            \"date-parts\": [\n",
      "                [\n",
      "                    2021,\n",
      "                    9,\n",
      "                    10\n",
      "                ]\n",
      "            ]\n",
      "        },\n",
      "        \"location\": \"Manchester, United Kingdom\",\n",
      "        \"name\": \"2021 IEEE International Smart Cities Conference (ISC2)\",\n",
      "        \"start\": {\n",
      "            \"date-parts\": [\n",
      "                [\n",
      "                    2021,\n",
      "                    9,\n",
      "                    7\n",
      "                ]\n",
      "            ]\n",
      "        }\n",
      "    },\n",
      "    \"funder\": [\n",
      "        {\n",
      "            \"DOI\": \"10.13039/501100001659\",\n",
      "            \"award\": [\n",
      "                \"1835\"\n",
      "            ],\n",
      "            \"doi-asserted-by\": \"publisher\",\n",
      "            \"name\": \"German Research Foundation (DFG)\"\n",
      "        }\n",
      "    ],\n",
      "    \"indexed\": {\n",
      "        \"date-parts\": [\n",
      "            [\n",
      "                2022,\n",
      "                5,\n",
      "                11\n",
      "            ]\n",
      "        ],\n",
      "        \"date-time\": \"2022-05-11T12:19:50Z\",\n",
      "        \"timestamp\": 1652271590617\n",
      "    },\n",
      "    \"is-referenced-by-count\": 0,\n",
      "    \"issued\": {\n",
      "        \"date-parts\": [\n",
      "            [\n",
      "                2021,\n",
      "                9,\n",
      "                7\n",
      "            ]\n",
      "        ]\n",
      "    },\n",
      "    \"license\": [\n",
      "        {\n",
      "            \"URL\": \"https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html\",\n",
      "            \"content-version\": \"vor\",\n",
      "            \"delay-in-days\": 0,\n",
      "            \"start\": {\n",
      "                \"date-parts\": [\n",
      "                    [\n",
      "                        2021,\n",
      "                        9,\n",
      "                        7\n",
      "                    ]\n",
      "                ],\n",
      "                \"date-time\": \"2021-09-07T00:00:00Z\",\n",
      "                \"timestamp\": 1630972800000\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"URL\": \"https://doi.org/10.15223/policy-029\",\n",
      "            \"content-version\": \"stm-asf\",\n",
      "            \"delay-in-days\": 0,\n",
      "            \"start\": {\n",
      "                \"date-parts\": [\n",
      "                    [\n",
      "                        2021,\n",
      "                        9,\n",
      "                        7\n",
      "                    ]\n",
      "                ],\n",
      "                \"date-time\": \"2021-09-07T00:00:00Z\",\n",
      "                \"timestamp\": 1630972800000\n",
      "            }\n",
      "        },\n",
      "        {\n",
      "            \"URL\": \"https://doi.org/10.15223/policy-037\",\n",
      "            \"content-version\": \"stm-asf\",\n",
      "            \"delay-in-days\": 0,\n",
      "            \"start\": {\n",
      "                \"date-parts\": [\n",
      "                    [\n",
      "                        2021,\n",
      "                        9,\n",
      "                        7\n",
      "                    ]\n",
      "                ],\n",
      "                \"date-time\": \"2021-09-07T00:00:00Z\",\n",
      "                \"timestamp\": 1630972800000\n",
      "            }\n",
      "        }\n",
      "    ],\n",
      "    \"link\": [\n",
      "        {\n",
      "            \"URL\": \"http://xplorestaging.ieee.org/ielx7/9562741/9562768/09562857.pdf?arnumber=9562857\",\n",
      "            \"content-type\": \"unspecified\",\n",
      "            \"content-version\": \"vor\",\n",
      "            \"intended-application\": \"similarity-checking\"\n",
      "        }\n",
      "    ],\n",
      "    \"member\": \"263\",\n",
      "    \"original-title\": [],\n",
      "    \"prefix\": \"10.1109\",\n",
      "    \"published\": {\n",
      "        \"date-parts\": [\n",
      "            [\n",
      "                2021,\n",
      "                9,\n",
      "                7\n",
      "            ]\n",
      "        ]\n",
      "    },\n",
      "    \"published-print\": {\n",
      "        \"date-parts\": [\n",
      "            [\n",
      "                2021,\n",
      "                9,\n",
      "                7\n",
      "            ]\n",
      "        ]\n",
      "    },\n",
      "    \"publisher\": \"IEEE\",\n",
      "    \"reference\": [\n",
      "        {\n",
      "            \"article-title\": \"The Kinetics Human Action Video Dataset\",\n",
      "            \"author\": \"kay\",\n",
      "            \"journal-title\": \"ar Xiv\",\n",
      "            \"key\": \"ref10\",\n",
      "            \"volume\": \"abs 1705 6950\",\n",
      "            \"year\": \"2017\"\n",
      "        },\n",
      "        {\n",
      "            \"article-title\": \"UCF101: A Dataset of 101 Human Actions Classes From Videos in The Wild\",\n",
      "            \"author\": \"soomro\",\n",
      "            \"journal-title\": \"CoRR\",\n",
      "            \"key\": \"ref11\",\n",
      "            \"volume\": \"abs 1212 402\",\n",
      "            \"year\": \"2012\"\n",
      "        },\n",
      "        {\n",
      "            \"article-title\": \"Image sequence based cyclist action recognition using multi-stream 3d convolution\",\n",
      "            \"author\": \"zernetsch\",\n",
      "            \"journal-title\": \"2020 International Conference on Pattern Recognition (ICPR) (accepted)\",\n",
      "            \"key\": \"ref12\",\n",
      "            \"year\": \"2020\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"zernetsch\",\n",
      "            \"journal-title\": \"Cyclist Actions Optical Flow Sequences and Trajectories\",\n",
      "            \"key\": \"ref13\",\n",
      "            \"year\": \"2020\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"zernetsch\",\n",
      "            \"key\": \"ref14\",\n",
      "            \"year\": \"2020\"\n",
      "        },\n",
      "        {\n",
      "            \"DOI\": \"10.1109/ITSC.2012.6338672\",\n",
      "            \"doi-asserted-by\": \"publisher\",\n",
      "            \"key\": \"ref15\"\n",
      "        },\n",
      "        {\n",
      "            \"article-title\": \"Speed/accuracy trade-offs for modern convolutional object detectors\",\n",
      "            \"author\": \"huang\",\n",
      "            \"journal-title\": \"CoRR\",\n",
      "            \"key\": \"ref16\",\n",
      "            \"volume\": \"abs 1611 10012\",\n",
      "            \"year\": \"2016\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"lin\",\n",
      "            \"first-page\": \"740\",\n",
      "            \"journal-title\": \"Microsoft coco Common objects in context\",\n",
      "            \"key\": \"ref17\",\n",
      "            \"year\": \"2014\"\n",
      "        },\n",
      "        {\n",
      "            \"DOI\": \"10.1109/CVPR.2018.00931\",\n",
      "            \"doi-asserted-by\": \"publisher\",\n",
      "            \"key\": \"ref18\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"bar-shalom\",\n",
      "            \"first-page\": \"267\",\n",
      "            \"journal-title\": \"Estimation for kinematic models\",\n",
      "            \"key\": \"ref19\",\n",
      "            \"year\": \"2001\"\n",
      "        },\n",
      "        {\n",
      "            \"DOI\": \"10.1109/IVS.2017.7995734\",\n",
      "            \"doi-asserted-by\": \"publisher\",\n",
      "            \"key\": \"ref4\"\n",
      "        },\n",
      "        {\n",
      "            \"DOI\": \"10.1109/CVPR.2016.110\",\n",
      "            \"doi-asserted-by\": \"publisher\",\n",
      "            \"key\": \"ref3\"\n",
      "        },\n",
      "        {\n",
      "            \"DOI\": \"10.1109/SSCI47803.2020.9308462\",\n",
      "            \"doi-asserted-by\": \"publisher\",\n",
      "            \"key\": \"ref6\"\n",
      "        },\n",
      "        {\n",
      "            \"DOI\": \"10.1109/TITS.2018.2836305\",\n",
      "            \"doi-asserted-by\": \"publisher\",\n",
      "            \"key\": \"ref5\"\n",
      "        },\n",
      "        {\n",
      "            \"DOI\": \"10.1109/CVPR.2019.00144\",\n",
      "            \"doi-asserted-by\": \"publisher\",\n",
      "            \"key\": \"ref8\"\n",
      "        },\n",
      "        {\n",
      "            \"DOI\": \"10.1109/TITS.2013.2280766\",\n",
      "            \"doi-asserted-by\": \"publisher\",\n",
      "            \"key\": \"ref7\"\n",
      "        },\n",
      "        {\n",
      "            \"DOI\": \"10.1109/IVS.2016.7535484\",\n",
      "            \"doi-asserted-by\": \"publisher\",\n",
      "            \"key\": \"ref2\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"goldhammer\",\n",
      "            \"journal-title\": \"Selbstlernende Algorithmen zur videobasierten Ab-sichtserkennung von Fu&#x00DF;g&#x00E4;ngern\",\n",
      "            \"key\": \"ref1\",\n",
      "            \"year\": \"2016\"\n",
      "        },\n",
      "        {\n",
      "            \"DOI\": \"10.1109/CVPR.2017.502\",\n",
      "            \"doi-asserted-by\": \"publisher\",\n",
      "            \"key\": \"ref9\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"abadi\",\n",
      "            \"journal-title\": \"TensorFlow Large-Scale Machine Learning on Heterogeneous Systems\",\n",
      "            \"key\": \"ref20\",\n",
      "            \"year\": \"2015\"\n",
      "        },\n",
      "        {\n",
      "            \"article-title\": \"Statistical comparisons of classifiers over multiple data sets\",\n",
      "            \"author\": \"demsar\",\n",
      "            \"first-page\": \"1\",\n",
      "            \"journal-title\": \"J Mach Learn Res\",\n",
      "            \"key\": \"ref22\",\n",
      "            \"volume\": \"7\",\n",
      "            \"year\": \"2006\"\n",
      "        },\n",
      "        {\n",
      "            \"author\": \"hollander\",\n",
      "            \"journal-title\": \"Nonparametric Statistical Methods Wiley Series in Probability and Statistics\",\n",
      "            \"key\": \"ref21\",\n",
      "            \"year\": \"2013\"\n",
      "        },\n",
      "        {\n",
      "            \"DOI\": \"10.1109/IVS.2019.8814258\",\n",
      "            \"doi-asserted-by\": \"publisher\",\n",
      "            \"key\": \"ref23\"\n",
      "        }\n",
      "    ],\n",
      "    \"reference-count\": 23,\n",
      "    \"references-count\": 23,\n",
      "    \"relation\": {},\n",
      "    \"resource\": {\n",
      "        \"primary\": {\n",
      "            \"URL\": \"https://ieeexplore.ieee.org/document/9562857/\"\n",
      "        }\n",
      "    },\n",
      "    \"score\": 1,\n",
      "    \"short-container-title\": [],\n",
      "    \"short-title\": [],\n",
      "    \"source\": \"Crossref\",\n",
      "    \"subtitle\": [],\n",
      "    \"title\": [\n",
      "        \"Cyclist Trajectory Forecasts by Incorporation of Multi-View Video Information\"\n",
      "    ],\n",
      "    \"type\": \"proceedings-article\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "print(json.dumps(dct, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb31fce0",
   "metadata": {},
   "source": [
    "E.g. we want to look up some author information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52ee4d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'given': 'Stefan',\n",
       " 'family': 'Zernetsch',\n",
       " 'sequence': 'first',\n",
       " 'affiliation': []}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "authors = dct.get('author')\n",
    "print(len(authors))\n",
    "authors[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57376b85",
   "metadata": {},
   "source": [
    "Get the abstract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77e4a971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "abstract = dct.get('abstract')\n",
    "print(abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a50ba52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: ['Cyclist Trajectory Forecasts by Incorporation of Multi-View Video Information'] \n",
      " ref_cout: 23 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_data(DOI):\n",
    "    dct = works.doi(DOI)\n",
    "    url = dct.get('URL')\n",
    "    cited_count = dct.get('is-referenced-by-count')\n",
    "    ref_count = dct.get('reference-count')\n",
    "    title = dct.get('title')\n",
    "    abstract = dct.get('abstract')\n",
    "    print('title:', title, '\\n ref_cout:', ref_count, '; cited_count:', cited_count, ';\\n abstract:', abstract, '\\n\\n')\n",
    "    \n",
    "    \n",
    "# doi = '10.1007/s10463-019-00720-8'\n",
    "get_data(DOI) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddb6ec7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ab266d6f",
   "metadata": {},
   "source": [
    "Get the references"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77a346cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references = dct.get('reference')\n",
    "len(references)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71ce18fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'key': 'ref14', 'author': 'zernetsch', 'year': '2020'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "references[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08d490fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "references[4].get('DOI')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cfdc45",
   "metadata": {},
   "source": [
    "Looping over the references we can display the DOI of each reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5b806c4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 :  None\n",
      "1 :  None\n",
      "2 :  None\n",
      "3 :  None\n",
      "4 :  None\n",
      "5 :  10.1109/ITSC.2012.6338672\n",
      "6 :  None\n",
      "7 :  None\n",
      "8 :  10.1109/CVPR.2018.00931\n",
      "9 :  None\n",
      "10 :  10.1109/IVS.2017.7995734\n",
      "11 :  10.1109/CVPR.2016.110\n",
      "12 :  10.1109/SSCI47803.2020.9308462\n",
      "13 :  10.1109/TITS.2018.2836305\n",
      "14 :  10.1109/CVPR.2019.00144\n",
      "15 :  10.1109/TITS.2013.2280766\n",
      "16 :  10.1109/IVS.2016.7535484\n",
      "17 :  None\n",
      "18 :  10.1109/CVPR.2017.502\n",
      "19 :  None\n",
      "20 :  None\n",
      "21 :  None\n",
      "22 :  10.1109/IVS.2019.8814258\n"
     ]
    }
   ],
   "source": [
    "for k in range(0, len(references)):\n",
    "    ref_doi = references[k].get('DOI')\n",
    "    print(k, ': ', ref_doi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5035aafd",
   "metadata": {},
   "source": [
    "Recall the possibility to download a \"pdf\" file by sci-hub, for a known DOI:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "55ce027d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:07:34 | Choose scihub url [0]: http://sci-hub.se\n",
      "[INFO] | 2022/10/21 02:07:38 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1109/ISC253183.2021.9562857]\n",
      "[INFO] | 2022/10/21 02:07:38 | -> Response: status_code=200, content_length=5839\n",
      "[WARNING] | 2022/10/21 02:07:38 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2022/10/21 02:07:38 | Choose scihub url [1]: https://sci-hub.st\n",
      "[INFO] | 2022/10/21 02:07:46 | <- Request: scihub_url=https://sci-hub.st, source=DoiSource[type=doi, id=10.1109/ISC253183.2021.9562857]\n",
      "[INFO] | 2022/10/21 02:07:46 | -> Response: status_code=200, content_length=6\n",
      "[WARNING] | 2022/10/21 02:07:46 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2022/10/21 02:07:46 | Choose scihub url [2]: https://sci-hub.ru\n",
      "[INFO] | 2022/10/21 02:07:50 | <- Request: scihub_url=https://sci-hub.ru, source=DoiSource[type=doi, id=10.1109/ISC253183.2021.9562857]\n",
      "[INFO] | 2022/10/21 02:07:50 | -> Response: status_code=200, content_length=5839\n",
      "[WARNING] | 2022/10/21 02:07:50 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2022/10/21 02:07:50 | Choose scihub url [3]: https://sci-hub.se\n",
      "[INFO] | 2022/10/21 02:07:52 | <- Request: scihub_url=https://sci-hub.se, source=DoiSource[type=doi, id=10.1109/ISC253183.2021.9562857]\n",
      "[INFO] | 2022/10/21 02:07:52 | -> Response: status_code=200, content_length=5839\n",
      "[WARNING] | 2022/10/21 02:07:52 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2022/10/21 02:07:52 | Choose scihub url [4]: http://sci-hub.ru\n",
      "[INFO] | 2022/10/21 02:07:56 | <- Request: scihub_url=http://sci-hub.ru, source=DoiSource[type=doi, id=10.1109/ISC253183.2021.9562857]\n",
      "[INFO] | 2022/10/21 02:07:56 | -> Response: status_code=200, content_length=5839\n",
      "[WARNING] | 2022/10/21 02:07:56 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[INFO] | 2022/10/21 02:07:56 | Choose scihub url [5]: http://sci-hub.st\n",
      "[INFO] | 2022/10/21 02:08:16 | <- Request: scihub_url=http://sci-hub.st, source=DoiSource[type=doi, id=10.1109/ISC253183.2021.9562857]\n",
      "[INFO] | 2022/10/21 02:08:16 | -> Response: status_code=200, content_length=24\n",
      "[WARNING] | 2022/10/21 02:08:16 | Error occurs, task status: extracting_failed, error: No pdf tag was found in the given content with the selector: #pdf\n",
      "[ERROR] | 2022/10/21 02:08:16 | Failed to download the paper: 10.1109/ISC253183.2021.9562857. Please try again.\n"
     ]
    }
   ],
   "source": [
    "from scidownl import scihub_download\n",
    "\n",
    "# paper = \"https://doi.org/10.1017/jfm.2017.541\"\n",
    "# doi = \"10.1017/jfm.2017.541\"\n",
    "out = \"./paper/\"\n",
    "# scihub_download(doi, paper_type=\"doi\")\n",
    "scihub_download(DOI, out=out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e53ed15",
   "metadata": {},
   "source": [
    "Now we can download all pdf files where the reference contains a DOI.\n",
    "We have to weed out the cases where a reference does not have a DOI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8d37398b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:12:14 | Choose scihub url [0]: http://sci-hub.se\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "---------------------------\n",
      "---------------------------\n",
      "---------------------------\n",
      "---------------------------\n",
      "5 10.1109/ITSC.2012.6338672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:12:18 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1109/ITSC.2012.6338672]\n",
      "[INFO] | 2022/10/21 02:12:18 | -> Response: status_code=200, content_length=7159\n",
      "[INFO] | 2022/10/21 02:12:18 | * Extracted information: {'url': 'https://zero.sci-hub.se/3100/ab168b16da802496a7d6405bcb434183/goldhammer2012.pdf', 'title': 'Cooperative multi sensor network for traffic safety applications at intersections. 2012 15th International IEEE Conference on Intelligent Transportation Systems'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80% [.............................................           ] 401408 / 497166"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:12:22 | ↓ Successfully download the url to: ./paper/Cooperative multi sensor network for traffic safety applications at intersections. 2012 15th International IEEE Conference on Intelligent Transportation Systems.pdf\n",
      "[INFO] | 2022/10/21 02:12:22 | Choose scihub url [0]: http://sci-hub.se\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 82% [..............................................          ] 409600 / 497166\r",
      " 84% [...............................................         ] 417792 / 497166\r",
      " 85% [...............................................         ] 425984 / 497166\r",
      " 87% [................................................        ] 434176 / 497166\r",
      " 88% [.................................................       ] 442368 / 497166\r",
      " 90% [..................................................      ] 450560 / 497166\r",
      " 92% [...................................................     ] 458752 / 497166\r",
      " 93% [....................................................    ] 466944 / 497166\r",
      " 95% [.....................................................   ] 475136 / 497166\r",
      " 97% [......................................................  ] 483328 / 497166\r",
      " 98% [....................................................... ] 491520 / 497166\r",
      "100% [........................................................] 497166 / 497166\n",
      "---------------------------\n",
      "---------------------------\n",
      "8 10.1109/CVPR.2018.00931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:12:27 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1109/CVPR.2018.00931]\n",
      "[INFO] | 2022/10/21 02:12:27 | -> Response: status_code=200, content_length=7025\n",
      "[INFO] | 2022/10/21 02:12:27 | * Extracted information: {'url': 'http://sci-hub.se/downloads/2019-08-27/ca/sun2018.pdf', 'title': 'PWC-Net  CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume. 2018 IEEE CVF Conference on Computer Vision and Pattern Recognition'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97% [....................................................  ] 1171456 / 1202365"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:12:46 | ↓ Successfully download the url to: ./paper/PWC-Net  CNNs for Optical Flow Using Pyramid, Warping, and Cost Volume. 2018 IEEE CVF Conference on Computer Vision and Pattern Recognition.pdf\n",
      "[INFO] | 2022/10/21 02:12:46 | Choose scihub url [0]: http://sci-hub.se\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 98% [....................................................  ] 1179648 / 1202365\r",
      " 98% [..................................................... ] 1187840 / 1202365\r",
      " 99% [..................................................... ] 1196032 / 1202365\r",
      "100% [......................................................] 1202365 / 1202365\n",
      "---------------------------\n",
      "10 10.1109/IVS.2017.7995734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:12:49 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1109/IVS.2017.7995734]\n",
      "[INFO] | 2022/10/21 02:12:49 | -> Response: status_code=200, content_length=6992\n",
      "[INFO] | 2022/10/21 02:12:49 | * Extracted information: {'url': 'https://zero.sci-hub.se/6525/bf293a26b35b8b5e76b6e7e67e24ed2e/pool2017.pdf', 'title': 'Using road topology to improve cyclist path prediction. 2017 IEEE Intelligent Vehicles Symposium (IV)'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 92% [...................................................     ] 172032 / 186181"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:12:53 | ↓ Successfully download the url to: ./paper/Using road topology to improve cyclist path prediction. 2017 IEEE Intelligent Vehicles Symposium (IV).pdf\n",
      "[INFO] | 2022/10/21 02:12:53 | Choose scihub url [0]: http://sci-hub.se\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 96% [......................................................  ] 180224 / 186181\r",
      "100% [........................................................] 186181 / 186181\n",
      "11 10.1109/CVPR.2016.110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:12:56 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1109/CVPR.2016.110]\n",
      "[INFO] | 2022/10/21 02:12:56 | -> Response: status_code=200, content_length=7073\n",
      "[INFO] | 2022/10/21 02:12:56 | * Extracted information: {'url': 'https://zero.sci-hub.se/6243/1da2ec1502c0a6eee787cb7d475a1b3d/alahi2016.pdf', 'title': 'Social LSTM  Human Trajectory Prediction in Crowded Spaces. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97% [....................................................  ] 1114112 / 1144725"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:13:20 | ↓ Successfully download the url to: ./paper/Social LSTM  Human Trajectory Prediction in Crowded Spaces. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).pdf\n",
      "[INFO] | 2022/10/21 02:13:20 | Choose scihub url [0]: http://sci-hub.se\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 98% [....................................................  ] 1122304 / 1144725\r",
      " 98% [..................................................... ] 1130496 / 1144725\r",
      " 99% [..................................................... ] 1138688 / 1144725\r",
      "100% [......................................................] 1144725 / 1144725\n",
      "12 10.1109/SSCI47803.2020.9308462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:13:24 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1109/SSCI47803.2020.9308462]\n",
      "[INFO] | 2022/10/21 02:13:24 | -> Response: status_code=200, content_length=7060\n",
      "[INFO] | 2022/10/21 02:13:24 | * Extracted information: {'url': 'http://sci-hub.se/downloads/2021-05-18/20/kress2020.pdf', 'title': 'Pose Based Action Recognition of Vulnerable Road Users Using Recurrent Neural Networks. 2020 IEEE Symposium Series on Computational Intelligence (SSCI)'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 80% [............................................            ] 221184 / 275666"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:13:29 | ↓ Successfully download the url to: ./paper/Pose Based Action Recognition of Vulnerable Road Users Using Recurrent Neural Networks. 2020 IEEE Symposium Series on Computational Intelligence (SSCI).pdf\n",
      "[INFO] | 2022/10/21 02:13:29 | Choose scihub url [0]: http://sci-hub.se\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 83% [..............................................          ] 229376 / 275666\r",
      " 86% [................................................        ] 237568 / 275666\r",
      " 89% [.................................................       ] 245760 / 275666\r",
      " 92% [...................................................     ] 253952 / 275666\r",
      " 95% [.....................................................   ] 262144 / 275666\r",
      " 98% [......................................................  ] 270336 / 275666\r",
      "100% [........................................................] 275666 / 275666\n",
      "13 10.1109/TITS.2018.2836305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:13:33 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1109/TITS.2018.2836305]\n",
      "[INFO] | 2022/10/21 02:13:33 | -> Response: status_code=200, content_length=7219\n",
      "[INFO] | 2022/10/21 02:13:33 | * Extracted information: {'url': 'https://zero.sci-hub.se/6900/508206da172651293475d7813e1cd7f7/quinterominguez2018.pdf', 'title': 'Pedestrian Path, Pose, and Intention Prediction Through Gaussian Process Dynamical Models and Pedestrian Activity Recognition. IEEE Transactions on Intelligent Transportation Systems, 1–12'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 51% [...........................                           ] 2121728 / 4105081"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING] | 2022/10/21 02:14:10 | Error occurs, task status: downloading_failed, error: <urlopen error retrieval incomplete: got only 2129589 out of 4105081 bytes>\n",
      "[INFO] | 2022/10/21 02:14:10 | Choose scihub url [1]: https://sci-hub.st\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 51% [............................                          ] 2129920 / 4105081"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:14:43 | <- Request: scihub_url=https://sci-hub.st, source=DoiSource[type=doi, id=10.1109/TITS.2018.2836305]\n",
      "[INFO] | 2022/10/21 02:14:43 | -> Response: status_code=200, content_length=19696\n",
      "[INFO] | 2022/10/21 02:14:43 | * Extracted information: {'url': 'https://twin.sci-hub.st/6900/508206da172651293475d7813e1cd7f7/quinterominguez2018.pdf#navpanes=0&view=FitH', 'title': 'Pedestrian Path, Pose, and Intention Prediction Through Gaussian Process Dynamical Models and Pedestrian Activity Recognition. IEEE Transactions on Intelligent Transportation Systems, 1–12'}\n",
      "[WARNING] | 2022/10/21 02:15:13 | Error occurs, task status: downloading_failed, error: HTTP Error 504: Gateway Time-out\n",
      "[INFO] | 2022/10/21 02:15:13 | Choose scihub url [2]: https://sci-hub.ru\n",
      "[INFO] | 2022/10/21 02:15:16 | <- Request: scihub_url=https://sci-hub.ru, source=DoiSource[type=doi, id=10.1109/TITS.2018.2836305]\n",
      "[INFO] | 2022/10/21 02:15:16 | -> Response: status_code=200, content_length=7219\n",
      "[INFO] | 2022/10/21 02:15:16 | * Extracted information: {'url': 'https://zero.sci-hub.ru/6900/508206da172651293475d7813e1cd7f7/quinterominguez2018.pdf', 'title': 'Pedestrian Path, Pose, and Intention Prediction Through Gaussian Process Dynamical Models and Pedestrian Activity Recognition. IEEE Transactions on Intelligent Transportation Systems, 1–12'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 99% [..................................................... ] 4071424 / 4105081"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:15:51 | ↓ Successfully download the url to: ./paper/Pedestrian Path, Pose, and Intention Prediction Through Gaussian Process Dynamical Models and Pedestrian Activity Recognition. IEEE Transactions on Intelligent Transportation Systems, 1–12.pdf\n",
      "[INFO] | 2022/10/21 02:15:51 | Choose scihub url [0]: http://sci-hub.se\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 99% [..................................................... ] 4079616 / 4105081\r",
      " 99% [..................................................... ] 4087808 / 4105081\r",
      " 99% [..................................................... ] 4096000 / 4105081\r",
      " 99% [..................................................... ] 4104192 / 4105081\r",
      "100% [......................................................] 4105081 / 4105081\n",
      "14 10.1109/CVPR.2019.00144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:15:57 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1109/CVPR.2019.00144]\n",
      "[INFO] | 2022/10/21 02:15:57 | -> Response: status_code=200, content_length=7136\n",
      "[INFO] | 2022/10/21 02:15:57 | * Extracted information: {'url': 'http://sci-hub.se/downloads/2020-06-03/58/sadeghian2019.pdf', 'title': 'SoPhie  An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints. 2019 IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR)'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 83% [.............................................         ] 1253376 / 1500183"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:16:07 | ↓ Successfully download the url to: ./paper/SoPhie  An Attentive GAN for Predicting Paths Compliant to Social and Physical Constraints. 2019 IEEE CVF Conference on Computer Vision and Pattern Recognition (CVPR).pdf\n",
      "[INFO] | 2022/10/21 02:16:07 | Choose scihub url [0]: http://sci-hub.se\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 84% [.............................................         ] 1261568 / 1500183\r",
      " 84% [.............................................         ] 1269760 / 1500183\r",
      " 85% [..............................................        ] 1277952 / 1500183\r",
      " 85% [..............................................        ] 1286144 / 1500183\r",
      " 86% [..............................................        ] 1294336 / 1500183\r",
      " 86% [..............................................        ] 1302528 / 1500183\r",
      " 87% [...............................................       ] 1310720 / 1500183\r",
      " 87% [...............................................       ] 1318912 / 1500183\r",
      " 88% [...............................................       ] 1327104 / 1500183\r",
      " 89% [................................................      ] 1335296 / 1500183\r",
      " 89% [................................................      ] 1343488 / 1500183\r",
      " 90% [................................................      ] 1351680 / 1500183\r",
      " 90% [................................................      ] 1359872 / 1500183\r",
      " 91% [.................................................     ] 1368064 / 1500183\r",
      " 91% [.................................................     ] 1376256 / 1500183\r",
      " 92% [.................................................     ] 1384448 / 1500183\r",
      " 92% [..................................................    ] 1392640 / 1500183\r",
      " 93% [..................................................    ] 1400832 / 1500183\r",
      " 93% [..................................................    ] 1409024 / 1500183\r",
      " 94% [...................................................   ] 1417216 / 1500183\r",
      " 95% [...................................................   ] 1425408 / 1500183\r",
      " 95% [...................................................   ] 1433600 / 1500183\r",
      " 96% [...................................................   ] 1441792 / 1500183\r",
      " 96% [....................................................  ] 1449984 / 1500183\r",
      " 97% [....................................................  ] 1458176 / 1500183\r",
      " 97% [....................................................  ] 1466368 / 1500183\r",
      " 98% [..................................................... ] 1474560 / 1500183\r",
      " 98% [..................................................... ] 1482752 / 1500183\r",
      " 99% [..................................................... ] 1490944 / 1500183\r",
      " 99% [..................................................... ] 1499136 / 1500183\r",
      "100% [......................................................] 1500183 / 1500183\n",
      "15 10.1109/TITS.2013.2280766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:16:12 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1109/TITS.2013.2280766]\n",
      "[INFO] | 2022/10/21 02:16:12 | -> Response: status_code=200, content_length=7057\n",
      "[INFO] | 2022/10/21 02:16:12 | * Extracted information: {'url': 'https://moscow.sci-hub.se/3184/061f3dc51bc2341b68bf031468076832/keller2014.pdf', 'title': 'Will the Pedestrian Cross  A Study on Pedestrian Path Prediction. IEEE Transactions on Intelligent Transportation Systems, 15(2), 494–506'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98% [..................................................... ] 1925120 / 1959495"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:16:58 | ↓ Successfully download the url to: ./paper/Will the Pedestrian Cross  A Study on Pedestrian Path Prediction. IEEE Transactions on Intelligent Transportation Systems, 15(2), 494–506.pdf\n",
      "[INFO] | 2022/10/21 02:16:58 | Choose scihub url [0]: http://sci-hub.se\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 98% [..................................................... ] 1933312 / 1959495\r",
      " 99% [..................................................... ] 1941504 / 1959495\r",
      " 99% [..................................................... ] 1949696 / 1959495\r",
      " 99% [..................................................... ] 1957888 / 1959495\r",
      "100% [......................................................] 1959495 / 1959495\n",
      "16 10.1109/IVS.2016.7535484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:17:03 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1109/IVS.2016.7535484]\n",
      "[INFO] | 2022/10/21 02:17:03 | -> Response: status_code=200, content_length=7086\n",
      "[INFO] | 2022/10/21 02:17:03 | * Extracted information: {'url': 'https://zero.sci-hub.se/6090/d425ae5368bf7d5458c62eb643f3ebed/zernetsch2016.pdf', 'title': 'Trajectory prediction of cyclists using a physical model and an artificial neural network. 2016 IEEE Intelligent Vehicles Symposium (IV)'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 78% [...........................................             ] 565248 / 721543"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:17:10 | ↓ Successfully download the url to: ./paper/Trajectory prediction of cyclists using a physical model and an artificial neural network. 2016 IEEE Intelligent Vehicles Symposium (IV).pdf\n",
      "[INFO] | 2022/10/21 02:17:10 | Choose scihub url [0]: http://sci-hub.se\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 79% [............................................            ] 573440 / 721543\r",
      " 80% [.............................................           ] 581632 / 721543\r",
      " 81% [.............................................           ] 589824 / 721543\r",
      " 82% [..............................................          ] 598016 / 721543\r",
      " 84% [...............................................         ] 606208 / 721543\r",
      " 85% [...............................................         ] 614400 / 721543\r",
      " 86% [................................................        ] 622592 / 721543\r",
      " 87% [................................................        ] 630784 / 721543\r",
      " 88% [.................................................       ] 638976 / 721543\r",
      " 89% [..................................................      ] 647168 / 721543\r",
      " 90% [..................................................      ] 655360 / 721543\r",
      " 91% [...................................................     ] 663552 / 721543\r",
      " 93% [....................................................    ] 671744 / 721543\r",
      " 94% [....................................................    ] 679936 / 721543\r",
      " 95% [.....................................................   ] 688128 / 721543\r",
      " 96% [......................................................  ] 696320 / 721543\r",
      " 97% [......................................................  ] 704512 / 721543\r",
      " 98% [....................................................... ] 712704 / 721543\r",
      " 99% [....................................................... ] 720896 / 721543\r",
      "100% [........................................................] 721543 / 721543\n",
      "---------------------------\n",
      "18 10.1109/CVPR.2017.502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:17:14 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1109/CVPR.2017.502]\n",
      "[INFO] | 2022/10/21 02:17:14 | -> Response: status_code=200, content_length=7047\n",
      "[INFO] | 2022/10/21 02:17:14 | * Extracted information: {'url': 'https://zero.sci-hub.se/6725/034b151063e58198bda2b2b05d753e88/carreira2017.pdf', 'title': 'Quo Vadis, Action Recognition  A New Model and the Kinetics Dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 97% [......................................................  ] 958464 / 978029"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:17:33 | ↓ Successfully download the url to: ./paper/Quo Vadis, Action Recognition  A New Model and the Kinetics Dataset. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR).pdf\n",
      "[INFO] | 2022/10/21 02:17:33 | Choose scihub url [0]: http://sci-hub.se\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 98% [....................................................... ] 966656 / 978029\r",
      " 99% [....................................................... ] 974848 / 978029\r",
      "100% [........................................................] 978029 / 978029\n",
      "---------------------------\n",
      "---------------------------\n",
      "---------------------------\n",
      "22 10.1109/IVS.2019.8814258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:17:37 | <- Request: scihub_url=http://sci-hub.se, source=DoiSource[type=doi, id=10.1109/IVS.2019.8814258]\n",
      "[INFO] | 2022/10/21 02:17:37 | -> Response: status_code=200, content_length=7058\n",
      "[INFO] | 2022/10/21 02:17:37 | * Extracted information: {'url': 'http://sci-hub.se/downloads/2019-09-09/bd/zernetsch2019.pdf', 'title': 'Trajectory Forecasts with Uncertainties of Vulnerable Road Users by Means of Neural Networks. 2019 IEEE Intelligent Vehicles Symposium (IV)'}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 98% [..................................................... ] 1941504 / 1971243"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] | 2022/10/21 02:18:07 | ↓ Successfully download the url to: ./paper/Trajectory Forecasts with Uncertainties of Vulnerable Road Users by Means of Neural Networks. 2019 IEEE Intelligent Vehicles Symposium (IV).pdf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      " 98% [..................................................... ] 1949696 / 1971243\r",
      " 99% [..................................................... ] 1957888 / 1971243\r",
      " 99% [..................................................... ] 1966080 / 1971243\r",
      "100% [......................................................] 1971243 / 1971243\n"
     ]
    }
   ],
   "source": [
    "for k in range(0, len(references)):\t\n",
    "    ref_doi=references[k].get('DOI')\n",
    "    if ref_doi != \"None\":\n",
    "        if(ref_doi):\n",
    "            print(k, ref_doi)\n",
    "            scihub_download(ref_doi, out=out)\n",
    "        else:\n",
    "            print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "380e723f",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b1b754d",
   "metadata": {},
   "source": [
    "...now a question is how we can also access to \"cross references\" that is DOI's articles that reference an article with a given DOI.\n",
    "\n",
    "\n",
    "We do allow searches for the counts against a DOI for cited by but at the moment there is no way in the REST API to search for cited by metadata for works, as members can only retrieve cited-by metadata for their own content.\n",
    "More information can be found here: https://www.crossref.org/services/cited-by/https://github.com/CrossRef/rest-api-doc/issues/374\n",
    "\n",
    "Not from Crossref, but from @opencitations there is COCI, the OpenCitations Index of Crossref open DOI-to-DOI references, cf. this blogpost https://opencitations.wordpress.com/2018/07/12/coci/ which allows such queries on the open part of the Crossref citation data.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c11591",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ae922d4",
   "metadata": {},
   "source": [
    "It seems that \"Opencitations\" is the better tool.\n",
    "https://github.com/opencitations\n",
    "https://opencitations.net/index/coci/api/v1#/citations/{doi}\n",
    "\n",
    "\n",
    "There are HTTP requests, which can be done from python.\n",
    "https://opencitations.net/index/coci/api/v1/references/10.1017/jfm.2017.541\n",
    "https://opencitations.net/index/coci/api/v1/citations/10.1017/jfm.2017.541"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78273ebc",
   "metadata": {},
   "source": [
    "Stillman, G. (2011). Applying Metacognitive Knowledge and Strategies in Applications and Modelling Tasks at Secondary School (pp. 165–180). https://doi.org/10.1007/978-94-007-0910-2_18\n",
    "\n",
    "Vorhölter, K. (2019). Enhancing metacognitive group strategies for modelling. ZDM Mathematics Education, 51(4), 703-716. https://doi.org/10.1007/s11858-019-01055-7\n",
    "\n",
    "Stillman, G. A., & Galbraith, P. L. (1998). Applying mathematics with real world connections: metacognitive characteristics of secondary students. Educational Studies in Mathematics, 36(2), 157–194. https://doi.org/10.1023/A:1003246329257\n",
    "\n",
    "Borromeo, R. (2006). Theoretical and empirical differentiations of phases in the modelling process. ZDM Mathematics Education, 38(2), 86–95. https://doi.org/10.1007/BF02655883\n",
    "\n",
    "\n",
    "Kaiser, G., & Schwarz, B. (2010). Authentic Modelling Problems in Mathematics Education- Examples and Experiences. Journal für Mathematik-Didaktik, 31(1), 51–76. https://doi.org/10.1007/s13138-010-0001-3\n",
    "\n",
    "Novoa-Muñoz F., 2019. GcCommunications in Statistics - Simulation and Computation. https://doi.org/10.1080/03610918.2019.1590598\n",
    "\n",
    "Wedelin, D., Adawi, T., Jahan, T., & Andersson, S. (2015). Investigating and developing engineering students’ mathematical modelling and problem-solving skills. European\n",
    "Journal of Engineering Education, 40(5), 557–572.\n",
    "https://doi.org/10.1080/03043797.2014.987648\n",
    "\n",
    "Novoa-Muñoz F., Jiménez-Gamero M.D., 2013. Testing for the bivariate Poisson distribution. Metrika. 77, 771-793.\n",
    "https://doi.org/10.1007/s00184-013-0464-6\n",
    "\n",
    "\n",
    "\n",
    "Meintanis S., Bassiakos Y., 2005. Goodness-of-fit tests for additively closed count models with an application to the generalized Hermite distribution. Sankhya. 67, 538–552. DOI:10.2307/25053448\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "27bb822c",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOI = \"10.1017/jfm.2017.541\"\n",
    "DOI = '10.1007/978-94-007-0910-2_18' # Stillman; book !\n",
    "\n",
    "DOI = 'doi.org/10.1007/s11858-019-01055-7' # Vorhölter; !\n",
    "DOI = '10.1023/A:1003246329257' # Stillman, G. A., & Galbraith; ERROR\n",
    "DOI = '10.1007/BF02655883' # Borromeo; - no abstract\n",
    "DOI = '10.1007/s13138-010-0001-3' # Kaiser, G., & Schwarz, B.\n",
    "DOI = '10.1007/BF02655885'\n",
    "DOI = '10.1007/BF02212307'\n",
    "DOI = '10.1080/03610918.2019.1590598' # Novoa-Muñoz; ERR: no tiene referencias\n",
    "DOI = '10.1080/03043797.2014.987648'\n",
    "DOI = '10.1007/s00184-013-0464-6' # Testing for the bivariate Poisson distribution; varias referencias, sin resumen reportado\n",
    "# DOI = '10.2307/25053448' # Meintanis S., Bassiakos Y.; sin referencias \n",
    "\n",
    "DOI = '10.1016/j.chb.2019.01.037' # Natalia\n",
    "# DOI = '10.22937/ijcsns.2022.22.4.37' # Felipe\n",
    "\n",
    "DOI = '10.1109/tits.2018.2836305' # 'Pedestrian Path, Pose, and Intention Prediction Through Gaussian Process Dynamical Models and Pedestrian Activity Recognition'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed0b0f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7c101b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# from requests import get\n",
    "\n",
    "HTTP_HEADERS = {\"authorization\": \"8bd01ec8-0c5e-44fc-9b56-c7b7565dd487\"}\n",
    "    \n",
    "def get_citing(DOI):\n",
    "    API_CALL_CIT= \"https://opencitations.net/index/coci/api/v1/citations/\"\n",
    "    API_CALL = API_CALL_CIT + DOI\n",
    "    HTTP_HEADERS = {\"authorization\": \"YOUR-OPENCITATIONS-ACCESS-TOKEN\"} # You can read the FAQs and get your token here: https://opencitations.net/accesstoken \n",
    "    HTTP_HEADERS = {\"authorization\": \"8bd01ec8-0c5e-44fc-9b56-c7b7565dd487\"}\n",
    "    response = requests.get(API_CALL, headers = HTTP_HEADERS)\n",
    "    response_dict = json.loads(response.text)\n",
    "    return response_dict\n",
    "    # print(json.dumps(response_dict, sort_keys=True, indent=4))\n",
    "\n",
    "    \n",
    "def get_cited(DOI):\n",
    "    API_CALL_REF = \"https://opencitations.net/index/coci/api/v1/references/\"\n",
    "    API_CALL = API_CALL_REF + DOI\n",
    "    HTTP_HEADERS = {\"authorization\": \"8bd01ec8-0c5e-44fc-9b56-c7b7565dd487\"}    \n",
    "    response = requests.get(API_CALL, headers = HTTP_HEADERS)\n",
    "    response_dict = json.loads(response.text)\n",
    "    return response_dict\n",
    "    # print(json.dumps(response_dict, sort_keys=True, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ee601f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_dict = get_citing(DOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b1c2df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ac87cb0",
   "metadata": {},
   "source": [
    "Figuring out the formate of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "42cdf25f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'journal_sc': 'no',\n",
       " 'creation': '2019-02-19',\n",
       " 'timespan': '-P0Y3M',\n",
       " 'cited': '10.1109/tits.2018.2836305',\n",
       " 'oci': '0200303090036280109000400080505-02001010009362918292837020001083702080306030005',\n",
       " 'citing': '10.3390/s19040855',\n",
       " 'author_sc': 'no'}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_dict[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daafefc",
   "metadata": {},
   "source": [
    "So, the citing url is accessible by the label \"citing\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bef9e084",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.3390/s19040855'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response_dict[0].get('citing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e701b76c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(response_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7861f805",
   "metadata": {},
   "source": [
    "....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d19d7f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract( 10.1109/tits.2018.2836305 ):\n",
      " None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# resumenes\n",
    "from crossref.restful import Works\n",
    "works = Works()\n",
    "\n",
    "def get_abstract(DOI):\n",
    "    dct = works.doi(DOI)\n",
    "    abstract = dct.get('abstract')\n",
    "    print('abstract(', DOI ,'):\\n', abstract,'\\n')\n",
    "    \n",
    "\n",
    "# doi = '10.1016/j.csda.2022.107548'\n",
    "get_abstract(DOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "89c7bd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show citing_\n",
    "def show_citing_articles(doi):\n",
    "    get_data(doi)\n",
    "    response_dict = get_citing(doi)\n",
    "    for k in range(0, len(response_dict)):\n",
    "        doi = response_dict[k].get('citing')\n",
    "        if(doi):\n",
    "            # r = get_citing(doi)\n",
    "            # print(k,': ', doi, '# cited', len(r))\n",
    "            get_data(doi) \n",
    "            get_abstract(doi)\n",
    "            # scihub_download(doi, out=out)\n",
    "        else:\n",
    "            print(\"---------------------------\")\n",
    "    return\n",
    "\n",
    "def show_cited_articles(doi):\n",
    "    get_data(doi)\n",
    "    get_abstract(doi)\n",
    "    response_dict = get_cited(doi)\n",
    "    for k in range(0, len(response_dict)):\n",
    "        doi = response_dict[k].get('cited')\n",
    "        if(doi):\n",
    "            r = get_citing(doi)\n",
    "            print(k,': ', doi, '# cited', len(r))\n",
    "            get_data(doi) \n",
    "            get_abstract(doi)\n",
    "            scihub_download(doi, out=out)\n",
    "        else:\n",
    "            print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "35a9f004",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: ['Pedestrian Path, Pose, and Intention Prediction Through Gaussian Process Dynamical Models and Pedestrian Activity Recognition'] \n",
      " ref_cout: 43 ; cited_count: 65 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "title: ['Vision Sensor Based Fuzzy System for Intelligent Vehicles'] \n",
      " ref_cout: 24 ; cited_count: 2 ;\n",
      " abstract: <jats:p>Those in the automotive industry and many researchers have become interested in the development of pedestrian protection systems in recent years. In particular, vision-based methods for predicting pedestrian intentions are now being actively studied to improve the performance of pedestrian protection systems. In this paper, we propose a vision-based system that can detect pedestrians using an on-dash camera in the car, and can then analyze their movements to determine the probability of collision. Information about pedestrians, including position, distance, movement direction, and magnitude are extracted using computer vision technologies and, using this information, a fuzzy rule-based system makes a judgement on the pedestrian’s risk level. To verify the function of the proposed system, we built several test datasets, collected by ourselves, in high-density regions where vehicles and pedestrians mix closely. The true positive rate of the experimental results was about 86%, which shows the validity of the proposed system.</jats:p> \n",
      "\n",
      "\n",
      "abstract( 10.3390/s19040855 ):\n",
      " <jats:p>Those in the automotive industry and many researchers have become interested in the development of pedestrian protection systems in recent years. In particular, vision-based methods for predicting pedestrian intentions are now being actively studied to improve the performance of pedestrian protection systems. In this paper, we propose a vision-based system that can detect pedestrians using an on-dash camera in the car, and can then analyze their movements to determine the probability of collision. Information about pedestrians, including position, distance, movement direction, and magnitude are extracted using computer vision technologies and, using this information, a fuzzy rule-based system makes a judgement on the pedestrian’s risk level. To verify the function of the proposed system, we built several test datasets, collected by ourselves, in high-density regions where vehicles and pedestrians mix closely. The true positive rate of the experimental results was about 86%, which shows the validity of the proposed system.</jats:p> \n",
      "\n",
      "title: ['Deep Learning and Statistical Models for Time-Critical Pedestrian Behaviour Prediction'] \n",
      " ref_cout: 23 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1007/978-3-030-36808-1_50 ):\n",
      " None \n",
      "\n",
      "title: ['Human motion trajectory prediction: a survey'] \n",
      " ref_cout: 303 ; cited_count: 177 ;\n",
      " abstract: <jats:p> With growing numbers of intelligent autonomous systems in human environments, the ability of such systems to perceive, understand, and anticipate human behavior becomes increasingly important. Specifically, predicting future positions of dynamic agents and planning considering such predictions are key tasks for self-driving vehicles, service robots, and advanced surveillance systems. This article provides a survey of human motion trajectory prediction. We review, analyze, and structure a large selection of work from different communities and propose a taxonomy that categorizes existing methods based on the motion modeling approach and level of contextual information used. We provide an overview of the existing datasets and performance metrics. We discuss limitations of the state of the art and outline directions for further research. </jats:p> \n",
      "\n",
      "\n",
      "abstract( 10.1177/0278364920917446 ):\n",
      " <jats:p> With growing numbers of intelligent autonomous systems in human environments, the ability of such systems to perceive, understand, and anticipate human behavior becomes increasingly important. Specifically, predicting future positions of dynamic agents and planning considering such predictions are key tasks for self-driving vehicles, service robots, and advanced surveillance systems. This article provides a survey of human motion trajectory prediction. We review, analyze, and structure a large selection of work from different communities and propose a taxonomy that categorizes existing methods based on the motion modeling approach and level of contextual information used. We provide an overview of the existing datasets and performance metrics. We discuss limitations of the state of the art and outline directions for further research. </jats:p> \n",
      "\n",
      "title: ['Efficient Behavior-aware Control of Automated Vehicles at Crosswalks using Minimal Information Pedestrian Prediction Model'] \n",
      " ref_cout: 24 ; cited_count: 2 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.23919/acc45564.2020.9147248 ):\n",
      " None \n",
      "\n",
      "title: ['A Framework for Modeling Knowledge Graphs via Processing Natural Descriptions of Vehicle-Pedestrian Interactions'] \n",
      " ref_cout: 20 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1007/978-3-030-59987-4_4 ):\n",
      " None \n",
      "\n",
      "title: ['Pedestrian Support in Intelligent Transportation Systems: Challenges, Solutions and Open issues'] \n",
      " ref_cout: 230 ; cited_count: 17 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1016/j.trc.2020.102856 ):\n",
      " None \n",
      "\n",
      "title: ['Adaptive lossless compression of skeleton sequences'] \n",
      " ref_cout: 69 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1016/j.image.2019.115659 ):\n",
      " None \n",
      "\n",
      "title: ['At the Traffic Intersection, Stopping, or Walking? Pedestrian Path Prediction Based on KPOF-GPDM for Driving Assistance'] \n",
      " ref_cout: 44 ; cited_count: 0 ;\n",
      " abstract: <jats:p>Pedestrian detection has always been a research hotspot in the Advanced Driving Assistance System (ADAS) with great progress in recent years. However, for the ADAS, we not only need to detect the behavior of pedestrians in front of the vehicle but also predict future action and the motion trajectory. Therefore, in this paper, we propose a human key point combined optical flow network (KPOF-Net) in the vehicle ADAS for the occlusion situation in the actual scene. When the vehicle encounters a blocked pedestrian at a traffic intersection, we used self-flow to estimate the global optical flow in the image sequence and then proposed a White Edge Cutting (WEC) algorithm to remove obstructions and simply modified the generative adversarial network to initialize pedestrians behind the obstructions. Next, we extracted pedestrian optical flow information and human joint point information in parallel, among which we trained four human key point models suitable for traffic intersections. At last, KPOF-GPDM fusion was proposed to predict the future status and walking trajectories of pedestrians, which combined optical flow information with human key point information. In the experiment, we did not merely compare our method with other four representative approaches in the same scene sequences. We also verified the accuracy of the pedestrian motion state and motion trajectory prediction of the system after fusion of human joint points and optical flow information. Taking into account the real-time performance of the system, in the low-speed and barrier-free environment, the comparative analysis only uses optical flow information, human joint point information, and KPOF-Net three prediction models. The results show that (1) in the same traffic environment, our proposed KPOF-Net can predict the change of pedestrian motion state about 5 frames (about 0.26 s) ahead of other excellent systems; (2) at the same time, our system predicts the trajectory of the pedestrian more accurately than the other four systems, which can achieve more stable minimum error ±0.04 m; (3) in a low-speed, barrier-free experimental environment, our proposed trajectory prediction model that integrates human joint points and optical flow information has higher prediction accuracy and smaller fluctuations than a single-information prediction model, and it can be well applied to automobiles’ ADAS.</jats:p> \n",
      "\n",
      "\n",
      "abstract( 10.1155/2021/9940126 ):\n",
      " <jats:p>Pedestrian detection has always been a research hotspot in the Advanced Driving Assistance System (ADAS) with great progress in recent years. However, for the ADAS, we not only need to detect the behavior of pedestrians in front of the vehicle but also predict future action and the motion trajectory. Therefore, in this paper, we propose a human key point combined optical flow network (KPOF-Net) in the vehicle ADAS for the occlusion situation in the actual scene. When the vehicle encounters a blocked pedestrian at a traffic intersection, we used self-flow to estimate the global optical flow in the image sequence and then proposed a White Edge Cutting (WEC) algorithm to remove obstructions and simply modified the generative adversarial network to initialize pedestrians behind the obstructions. Next, we extracted pedestrian optical flow information and human joint point information in parallel, among which we trained four human key point models suitable for traffic intersections. At last, KPOF-GPDM fusion was proposed to predict the future status and walking trajectories of pedestrians, which combined optical flow information with human key point information. In the experiment, we did not merely compare our method with other four representative approaches in the same scene sequences. We also verified the accuracy of the pedestrian motion state and motion trajectory prediction of the system after fusion of human joint points and optical flow information. Taking into account the real-time performance of the system, in the low-speed and barrier-free environment, the comparative analysis only uses optical flow information, human joint point information, and KPOF-Net three prediction models. The results show that (1) in the same traffic environment, our proposed KPOF-Net can predict the change of pedestrian motion state about 5 frames (about 0.26 s) ahead of other excellent systems; (2) at the same time, our system predicts the trajectory of the pedestrian more accurately than the other four systems, which can achieve more stable minimum error ±0.04 m; (3) in a low-speed, barrier-free experimental environment, our proposed trajectory prediction model that integrates human joint points and optical flow information has higher prediction accuracy and smaller fluctuations than a single-information prediction model, and it can be well applied to automobiles’ ADAS.</jats:p> \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: ['Roadside pedestrian motion prediction using Bayesian methods and particle filter'] \n",
      " ref_cout: 46 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1049/itr2.12090 ):\n",
      " None \n",
      "\n",
      "title: ['An Online Semisupervised Learning Model for Pedestrians’ Crossing Intention Recognition of Connected Autonomous Vehicle Based on Mobile Edge Computing Applications'] \n",
      " ref_cout: 38 ; cited_count: 0 ;\n",
      " abstract: <jats:p>One of the major challenges that connected autonomous vehicles (CAVs) are facing today is driving in urban environments. To achieve this goal, CAVs need to have the ability to understand the crossing intention of pedestrians. However, for autonomous vehicles, it is quite challenging to understand pedestrians’ crossing intentions. Because the pedestrian is a very complex individual, their intention to cross the street is affected by the weather, the surrounding traffic environment, and even his own emotions. If the established street crossing intention recognition model cannot be updated in real time according to the diversity of samples, the efficiency of human-machine interaction and the interaction safety will be greatly affected. Based on the above problems, this paper established a pedestrian crossing intention model based on the online semisupervised support vector machine algorithm (OS3VM). In order to verify the effectiveness of the model, this paper collects a large amount of pedestrian crossing data and vehicle movement data based on laser scanner, and determines the main feature components of the model input through feature extraction and principal component analysis (PCA). The comparison results of recognition accuracy of SVM, S3VM, and OS3VM indicate that the proposed OS3VM model exhibits a better ability to recognize pedestrian crossing intentions than the SVM and S3VM models, and the accuracy achieves 94.83%. Therefore, the OS3VM model can reduce the number of labeled samples for training the classifier and improve the recognition accuracy.</jats:p> \n",
      "\n",
      "\n",
      "abstract( 10.1155/2021/6621451 ):\n",
      " <jats:p>One of the major challenges that connected autonomous vehicles (CAVs) are facing today is driving in urban environments. To achieve this goal, CAVs need to have the ability to understand the crossing intention of pedestrians. However, for autonomous vehicles, it is quite challenging to understand pedestrians’ crossing intentions. Because the pedestrian is a very complex individual, their intention to cross the street is affected by the weather, the surrounding traffic environment, and even his own emotions. If the established street crossing intention recognition model cannot be updated in real time according to the diversity of samples, the efficiency of human-machine interaction and the interaction safety will be greatly affected. Based on the above problems, this paper established a pedestrian crossing intention model based on the online semisupervised support vector machine algorithm (OS3VM). In order to verify the effectiveness of the model, this paper collects a large amount of pedestrian crossing data and vehicle movement data based on laser scanner, and determines the main feature components of the model input through feature extraction and principal component analysis (PCA). The comparison results of recognition accuracy of SVM, S3VM, and OS3VM indicate that the proposed OS3VM model exhibits a better ability to recognize pedestrian crossing intentions than the SVM and S3VM models, and the accuracy achieves 94.83%. Therefore, the OS3VM model can reduce the number of labeled samples for training the classifier and improve the recognition accuracy.</jats:p> \n",
      "\n",
      "title: ['Stochastic-Biomechanic Modeling and Recognition of Human Movement Primitives, in Industry, Using Wearables'] \n",
      " ref_cout: 50 ; cited_count: 4 ;\n",
      " abstract: <jats:p>In industry, ergonomists apply heuristic methods to determine workers’ exposure to ergonomic risks; however, current methods are limited to evaluating postures or measuring the duration and frequency of professional tasks. The work described here aims to deepen ergonomic analysis by using joint angles computed from inertial sensors to model the dynamics of professional movements and the collaboration between joints. This work is based on the hypothesis that with these models, it is possible to forecast workers’ posture and identify the joints contributing to the motion, which can later be used for ergonomic risk prevention. The modeling was based on the Gesture Operational Model, which uses autoregressive models to learn the dynamics of the joints by assuming associations between them. Euler angles were used for training to avoid forecasting errors such as bone stretching and invalid skeleton configurations, which commonly occur with models trained with joint positions. The statistical significance of the assumptions of each model was computed to determine the joints most involved in the movements. The forecasting performance of the models was evaluated, and the selection of joints was validated, by achieving a high gesture recognition performance. Finally, a sensitivity analysis was conducted to investigate the response of the system to disturbances and their effect on the posture.</jats:p> \n",
      "\n",
      "\n",
      "abstract( 10.3390/s21072497 ):\n",
      " <jats:p>In industry, ergonomists apply heuristic methods to determine workers’ exposure to ergonomic risks; however, current methods are limited to evaluating postures or measuring the duration and frequency of professional tasks. The work described here aims to deepen ergonomic analysis by using joint angles computed from inertial sensors to model the dynamics of professional movements and the collaboration between joints. This work is based on the hypothesis that with these models, it is possible to forecast workers’ posture and identify the joints contributing to the motion, which can later be used for ergonomic risk prevention. The modeling was based on the Gesture Operational Model, which uses autoregressive models to learn the dynamics of the joints by assuming associations between them. Euler angles were used for training to avoid forecasting errors such as bone stretching and invalid skeleton configurations, which commonly occur with models trained with joint positions. The statistical significance of the assumptions of each model was computed to determine the joints most involved in the movements. The forecasting performance of the models was evaluated, and the selection of joints was validated, by achieving a high gesture recognition performance. Finally, a sensitivity analysis was conducted to investigate the response of the system to disturbances and their effect on the posture.</jats:p> \n",
      "\n",
      "title: ['Semantic Synthesis of Pedestrian Locomotion'] \n",
      " ref_cout: 63 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1007/978-3-030-69532-3_29 ):\n",
      " None \n",
      "\n",
      "title: ['Pedestrian intention prediction: A convolutional bottom-up multi-task approach'] \n",
      " ref_cout: 82 ; cited_count: 6 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1016/j.trc.2021.103259 ):\n",
      " None \n",
      "\n",
      "title: ['Pose Based Trajectory Forecast of Vulnerable Road Users Using Recurrent Neural Networks'] \n",
      " ref_cout: 22 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1007/978-3-030-68763-2_5 ):\n",
      " None \n",
      "\n",
      "title: ['Statistical models of near-accident event and pedestrian behavior at non-signalized intersections'] \n",
      " ref_cout: 34 ; cited_count: 24 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1080/02664763.2021.1962263 ):\n",
      " None \n",
      "\n",
      "title: ['An improved GAN with transformers for pedestrian trajectory prediction models'] \n",
      " ref_cout: 52 ; cited_count: 1 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1002/int.22724 ):\n",
      " None \n",
      "\n",
      "title: ['Efficient Histogram-Based Gradient Boosting Approach for Accident Severity Prediction With Multisource Data'] \n",
      " ref_cout: 89 ; cited_count: 1 ;\n",
      " abstract: <jats:p> Many people lose their lives in road accidents because they do not receive timely treatment after the accident from emergency medical services; providing timely emergency services can decrease the fatality rate as well as the severity of accidents. In this study, we predicted the severity of car accidents for use by trauma centers and hospitals for emergency response management. The predictions of our model could be used to decide whether an ambulance unit should be dispatched to the crash site or not. This study used histogram-based gradient boosting (HistGBDT), a modification of the gradient boosting (GBDT) classifier that accelerates the learning process and increases a model’s prediction power. The HistGBDT model was compared with seven state-of-the-art machine learning models: logistic regression, multilayer perceptron, random forest, extremely randomized trees, bagging, AdaBoost, and GBDT. The experiments were conducted on French accident data from 2005 to 2018. The HistGBDT model, with an overall accuracy of 82.5%, recall of 76.7%, and precision of 81.9%, outperformed other models. An analysis of feature importance indicated that safety equipment was the most important feature and vehicle category, department, localization, and region were other significant features. The Fβ measure (i.e., the weighted harmonic mean of recall and precision) was optimized with different weights on recall for the four best performing models to compare the tradeoff between the two crucial performance measures. </jats:p> \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract( 10.1177/03611981221074370 ):\n",
      " <jats:p> Many people lose their lives in road accidents because they do not receive timely treatment after the accident from emergency medical services; providing timely emergency services can decrease the fatality rate as well as the severity of accidents. In this study, we predicted the severity of car accidents for use by trauma centers and hospitals for emergency response management. The predictions of our model could be used to decide whether an ambulance unit should be dispatched to the crash site or not. This study used histogram-based gradient boosting (HistGBDT), a modification of the gradient boosting (GBDT) classifier that accelerates the learning process and increases a model’s prediction power. The HistGBDT model was compared with seven state-of-the-art machine learning models: logistic regression, multilayer perceptron, random forest, extremely randomized trees, bagging, AdaBoost, and GBDT. The experiments were conducted on French accident data from 2005 to 2018. The HistGBDT model, with an overall accuracy of 82.5%, recall of 76.7%, and precision of 81.9%, outperformed other models. An analysis of feature importance indicated that safety equipment was the most important feature and vehicle category, department, localization, and region were other significant features. The Fβ measure (i.e., the weighted harmonic mean of recall and precision) was optimized with different weights on recall for the four best performing models to compare the tradeoff between the two crucial performance measures. </jats:p> \n",
      "\n",
      "title: ['Data Driven Models for Human Motion Prediction in Human-Robot Collaboration'] \n",
      " ref_cout: 56 ; cited_count: 7 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/access.2020.3045994 ):\n",
      " None \n",
      "\n",
      "title: ['Crossing or Not? Context-Based Recognition of Pedestrian Crossing Intention in the Urban Environment'] \n",
      " ref_cout: 47 ; cited_count: 7 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/tits.2021.3053031 ):\n",
      " None \n",
      "\n",
      "title: ['Pedestrian Motion Trajectory Prediction in Intelligent Driving from Far Shot First-Person Perspective Video'] \n",
      " ref_cout: 47 ; cited_count: 33 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/tits.2021.3052908 ):\n",
      " None \n",
      "\n",
      "title: ['A Spatio-Temporal Multilayer Perceptron for Gesture Recognition'] \n",
      " ref_cout: 55 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/iv51971.2022.9827054 ):\n",
      " None \n",
      "\n",
      "title: ['A Dynamic Spatial-Temporal Attention Network for Early Anticipation of Traffic Accidents'] \n",
      " ref_cout: 32 ; cited_count: 2 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/tits.2022.3155613 ):\n",
      " None \n",
      "\n",
      "title: ['Cyclist Trajectory Forecasts by Incorporation of Multi-View Video Information'] \n",
      " ref_cout: 23 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/isc253183.2021.9562857 ):\n",
      " None \n",
      "\n",
      "title: ['Automated Braking Decision and Control for Pedestrian Collision Avoidance Based on Risk Assessment'] \n",
      " ref_cout: 34 ; cited_count: 1 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/mits.2021.3098618 ):\n",
      " None \n",
      "\n",
      "title: ['Estimating pedestrian intentions from trajectory data'] \n",
      " ref_cout: 15 ; cited_count: 1 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/iccp48234.2019.8959707 ):\n",
      " None \n",
      "\n",
      "title: ['Pedestrian Trajectory Prediction Combining Probabilistic Reasoning and Sequence Learning'] \n",
      " ref_cout: 48 ; cited_count: 4 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/tiv.2020.2966117 ):\n",
      " None \n",
      "\n",
      "title: ['Image Sequence Based Cyclist Action Recognition Using Multi-Stream 3D Convolution'] \n",
      " ref_cout: 27 ; cited_count: 1 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/icpr48806.2021.9413233 ):\n",
      " None \n",
      "\n",
      "title: ['A Data-driven Markovian Framework for Multi-agent Pedestrian Collision Risk Prediction'] \n",
      " ref_cout: 11 ; cited_count: 1 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/itsc.2019.8917142 ):\n",
      " None \n",
      "\n",
      "title: ['Pedestrian Crossing Intention Prediction at Red-Light Using Pose Estimation'] \n",
      " ref_cout: 59 ; cited_count: 3 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/tits.2021.3074829 ):\n",
      " None \n",
      "\n",
      "title: ['Mapping pedestrian safety studies between 2010 and 2021: A scientometric analysis'] \n",
      " ref_cout: 67 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1016/j.aap.2022.106744 ):\n",
      " None \n",
      "\n",
      "title: ['Model for deep learning-based skill transfer in an assembly process'] \n",
      " ref_cout: 59 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1016/j.aei.2022.101643 ):\n",
      " None \n",
      "\n",
      "title: ['Recognition of Pedestrians’ Crossing Intentions with a Conv-Transformer Network'] \n",
      " ref_cout: 24 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1007/978-3-031-06767-9_39 ):\n",
      " None \n",
      "\n",
      "title: ['Pedestrian Intent Detection using Skeleton-based Prediction for Road Safety'] \n",
      " ref_cout: 8 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/access51619.2021.9563293 ):\n",
      " None \n",
      "\n",
      "title: ['Congestion-aware Multi-agent Trajectory Prediction for Collision Avoidance'] \n",
      " ref_cout: 90 ; cited_count: 2 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/icra48506.2021.9560994 ):\n",
      " None \n",
      "\n",
      "title: ['Autonomous Vehicle Evaluation: A Comprehensive Survey on Modeling and Simulation Approaches'] \n",
      " ref_cout: 377 ; cited_count: 6 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/access.2021.3125620 ):\n",
      " None \n",
      "\n",
      "title: ['Analysis and Prediction of Pedestrian Crosswalk Behavior during Automated Vehicle Interactions'] \n",
      " ref_cout: 37 ; cited_count: 7 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/icra40945.2020.9197347 ):\n",
      " None \n",
      "\n",
      "title: ['Pedestrian Group Detection with K-Means and DBSCAN Clustering Methods'] \n",
      " ref_cout: 26 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/eit53891.2022.9813918 ):\n",
      " None \n",
      "\n",
      "title: ['Comparison of the Pedestrian Crossing Intention Parameters and Research on Intention Recognition Model Under Different Road Conditions'] \n",
      " ref_cout: 36 ; cited_count: 1 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/access.2020.3034236 ):\n",
      " None \n",
      "\n",
      "title: ['TITAN: Future Forecast Using Action Priors'] \n",
      " ref_cout: 60 ; cited_count: 25 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/cvpr42600.2020.01120 ):\n",
      " None \n",
      "\n",
      "title: ['Pedestrian Path Prediction for Autonomous Driving at Un-Signalized Crosswalk Using W/CDM and MSFM'] \n",
      " ref_cout: 35 ; cited_count: 7 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/tits.2020.2979231 ):\n",
      " None \n",
      "\n",
      "title: ['Intention Recognition of Pedestrians and Cyclists by 2D Pose Estimation'] \n",
      " ref_cout: 38 ; cited_count: 31 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/tits.2019.2946642 ):\n",
      " None \n",
      "\n",
      "title: ['A Deep Learning-Based Hybrid Framework for Object Detection and Recognition in Autonomous Driving'] \n",
      " ref_cout: 31 ; cited_count: 31 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/access.2020.3033289 ):\n",
      " None \n",
      "\n",
      "title: ['A Survey on Motion Prediction of Pedestrians and Vehicles for Autonomous Driving'] \n",
      " ref_cout: 62 ; cited_count: 3 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/access.2021.3118224 ):\n",
      " None \n",
      "\n",
      "title: ['Skeleton-based traffic command recognition at road intersections for intelligent vehicles'] \n",
      " ref_cout: 42 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1016/j.neucom.2022.05.107 ):\n",
      " None \n",
      "\n",
      "title: ['Using Appearance to Predict Pedestrian Trajectories Through Disparity-Guided Attention and Convolutional LSTM'] \n",
      " ref_cout: 53 ; cited_count: 2 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/tvt.2021.3094678 ):\n",
      " None \n",
      "\n",
      "title: ['Coarse-to-Fine Deep Learning of Continuous Pedestrian Orientation Based on Spatial Co-Occurrence Feature'] \n",
      " ref_cout: 48 ; cited_count: 4 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/tits.2019.2919920 ):\n",
      " None \n",
      "\n",
      "title: ['Pose Based Trajectory Forecast of Vulnerable Road Users'] \n",
      " ref_cout: 20 ; cited_count: 2 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/ssci44817.2019.9003023 ):\n",
      " None \n",
      "\n",
      "title: ['Multimodal Hybrid Pedestrian: A Hybrid Automaton Model of Urban Pedestrian Behavior for Automated Driving Applications'] \n",
      " ref_cout: 74 ; cited_count: 5 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/access.2021.3058307 ):\n",
      " None \n",
      "\n",
      "title: ['Pedestrian Models for Autonomous Driving Part II: High-Level Models of Human Behavior'] \n",
      " ref_cout: 232 ; cited_count: 26 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/tits.2020.3006767 ):\n",
      " None \n",
      "\n",
      "title: ['Pedestrian Skeleton Tracking Using OpenPose and Probabilistic Filtering'] \n",
      " ref_cout: 15 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/argencon49523.2020.9505458 ):\n",
      " None \n",
      "\n",
      "title: ['RNN-based Pedestrian Crossing Prediction using Activity and Pose-related Features'] \n",
      " ref_cout: 28 ; cited_count: 5 ;\n",
      " abstract: None \n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abstract( 10.1109/iv47402.2020.9304652 ):\n",
      " None \n",
      "\n",
      "title: ['Pedestrian Motion State Estimation From 2D Pose'] \n",
      " ref_cout: 25 ; cited_count: 0 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/iv47402.2020.9304784 ):\n",
      " None \n",
      "\n",
      "title: ['Pedestrian-Aware Statistical Risk Assessment'] \n",
      " ref_cout: 36 ; cited_count: 34 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/tits.2021.3074522 ):\n",
      " None \n",
      "\n",
      "title: ['Pose Based Action Recognition of Vulnerable Road Users Using Recurrent Neural Networks'] \n",
      " ref_cout: 21 ; cited_count: 2 ;\n",
      " abstract: None \n",
      "\n",
      "\n",
      "abstract( 10.1109/ssci47803.2020.9308462 ):\n",
      " None \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# get_cited\n",
    "# doi = '10.1006/jmva.2001.2041'\n",
    "\n",
    "DOI = '10.1109/tits.2018.2836305' # ['Pedestrian Path, Pose, and Intention Prediction Through Gaussian Process Dynamical Models and Pedestrian Activity Recognition'] \n",
    "\n",
    "# show_cited_articles(DOI)\n",
    "show_citing_articles(DOI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b56e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = '10.1137/1138055'\n",
    "get_data(doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0648a9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f51e09",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b89082e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc62f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = '10.1016/j.chb.2019.01.037' # Natalia\n",
    "# doi = '10.1016/j.cose.2020.102003' # Felipe\n",
    "show_citing_articles(doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91995883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f00343",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = '10.1016/0005-7967(93)90094-b'\n",
    "show_citing_articles(doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd0e0d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = '10.1016/j.ijhcs.2020.102503'\n",
    "show_citing_articles(doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9ac385",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = '10.1177/0887302x11411709'\n",
    "show_citing_articles(doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7dd0f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = '10.1016/j.jclepro.2017.06.128'\n",
    "show_citing_articles(doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f22af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = '10.1016/j.jclepro.2020.121026'\n",
    "show_citing_articles(doi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4b511b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = '10.1007/s10668-020-00844-5'\n",
    "show_citing_articles(doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe6f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "doi = '10.3390/su13116219'\n",
    "show_citing_articles(doi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a069e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfdce37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12287589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640a8d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9acc29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in range(0, len(response_dict)):\n",
    "    doi = response_dict[k].get('citing')\n",
    "    if(doi):\n",
    "        print(k,': ', doi)\n",
    "        get_abstract(doi)\n",
    "        # scihub_download(doi, out=out)\n",
    "    else:\n",
    "        print(\"---------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf02c2d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22117120",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed194fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c6dda3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8baa900a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9e611b5f",
   "metadata": {},
   "source": [
    "It works for both citations and references.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50c549",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cfb3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35688a93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b70d7bfa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d9fedb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9dae65b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1792e7e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd73b64d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed49b39c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da08ea50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc813c1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "98f30860",
   "metadata": {},
   "source": [
    "Here some possiblities for queries with \"crossrefs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3002dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "works = Works()\n",
    "works.query('design thinking').url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdcf0709",
   "metadata": {},
   "outputs": [],
   "source": [
    "works.query('design thinking').filter(from_online_pub_date='2020').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1232db45",
   "metadata": {},
   "outputs": [],
   "source": [
    "wq=works.query('design thinking').filter(from_online_pub_date='2020')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d957b3ea",
   "metadata": {},
   "source": [
    "Other demos\n",
    "* https://pypi.org/project/crossrefapi/1.0.3/\n",
    "* https://github.com/CrossRef/rest-api-doc/blob/master/demos/crossref-api-demo.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83cab23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ddaaff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459697ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a23b5d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10b9f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72abdf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bf9740",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
